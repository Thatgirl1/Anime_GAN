"""""
In order to better train the DCGAN network, 
we generally divide the training into two parts according to the ideas in the paper.

The first part: Training discriminator, 
requires maximization to correctly classify a given input as true (label = 1) or false (label = 0), 
which means we want the maximum loss:log (D (x)) + log (1-D (G (z))). 
Where log (D (x)) represents the actual real sample loss and log (1-D (G (z))) is the fake sample loss generated by the generator.

The second part: Training generator, 
which is required to train the generator by minimizing log (1-D (G (z))), 
in order to generate a more perfect sample, to achieve the purpose of changing false to true
But Goodfellow shows that this does not provide enough gradients, especially in the early stages of the learning process. 
As a modification, we want to maximize log (D (G (z))).
Classify the Generator output in Part 1, calculate the loss of G using the real label as the GT, 
calculate the gradient of G in the reverse pass, and finally use the optimizer step to update the parameters of G.
"""""
#let's go on.loading the head file firstly

import argparse
import torch
import torchvision
import torchvision.utils as vutils
import torch.nn as nn
from model import NetD, NetG

#Here, I will load the argparse module to define and use some parameters that need to be set.
parser = argparse.ArgumentParser()
parser.add_argument('--batchSize', type=int, default=64)
parser.add_argument('--imageSize', type=int, default=96,help='Training image data size')
parser.add_argument('--epoch', type=int, default=25, help='number of epochs to train for')
parser.add_argument('--lr', type=float, default=0.0002, help='learning rate, default=0.0002')
parser.add_argument('--nz', type=int, default=100, help='size of the latent z vector')
parser.add_argument('--ngf', type=int, default=64)
parser.add_argument('--ndf', type=int, default=64)
parser.add_argument('--model_path', default='./model_save/',help='Model save location')
parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')
parser.add_argument('--data_path', default='./train_data/', help='folder to train data')
parser.add_argument('--output', default='./Generate_image/', help='folder to output images and model checkpoints')
opt = parser.parse_args()

# Get whether there is GPU acceleration locally, otherwise use CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#Preprocessing images using tramsforms
transforms = torchvision.transforms.Compose([
    torchvision.transforms.Resize(opt.imageSize),
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

dataset = torchvision.datasets.ImageFolder(opt.data_path, transform=transforms)

dataloader = torch.utils.data.DataLoader(dataset=dataset,batch_size=opt.batchSize,shuffle=True,drop_last=True)

netG = NetG(opt.ngf, opt.nz).to(device)
netD = NetD(opt.ndf).to(device)

criterion = nn.BCELoss()
optimizerG = torch.optim.Adam(netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))
optimizerD = torch.optim.Adam(netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))

label = torch.FloatTensor(opt.batchSize)
real_label = 1
fake_label = 0

if __name__ == '__main__':
    print("start training the model")
    for epoch in range(1, opt.epoch + 1):
        for i, (imgs, _) in enumerate(dataloader):
            # Step 1: Fix generator G and train discriminator D
            optimizerD.zero_grad()
            imgs = imgs.to(device)
            output = netD(imgs)
            label.data.fill_(real_label)
            label = label.to(device)
            errD_real = criterion(output, label)
            errD_real.backward()
            label.data.fill_(fake_label)
            noise = torch.randn(opt.batchSize, opt.nz, 1, 1)
            noise = noise.to(device)
            fake = netG(noise)  # Generate fake map
            output = netD(fake.detach())
            errD_fake = criterion(output, label)
            errD_fake.backward()
            errD = errD_fake + errD_real
            optimizerD.step()

            # Step 2: Fix discriminator D and train generator G
            optimizerG.zero_grad()
            label.data.fill_(real_label)
            label = label.to(device)
            output = netD(fake)
            errG = criterion(output, label)
            errG.backward()
            optimizerG.step()

            print('[%d/%d][%d/%d] Loss_D: %.3f Loss_G %.3f'
                  % (epoch, opt.epoch, i, len(dataloader), errD.item(), errG.item()))

        vutils.save_image(fake.data,
                          '%s/fake_samples_epoch_%03d.png' % (opt.output, epoch),
                          normalize=True)
        torch.save(netG.state_dict(), '%s/netG_%03d.pth' % (opt.model_path, epoch))
        torch.save(netD.state_dict(), '%s/netD_%03d.pth' % (opt.model_path, epoch))

    print("end training")